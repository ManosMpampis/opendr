{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-04T10:30:16.927219887Z",
     "start_time": "2023-07-04T10:30:15.256079809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 13:30:15.539903: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-04 13:30:15.561147: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from tbparse import SummaryReader\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def combine_tensorboard_graphs(log_dir, experiment_name):\n",
    "    text_tag = \"config\"\n",
    "    dataFrames = []\n",
    "    dirs = sorted(os.listdir(f\"{log_dir}/{experiment_name}\"))\n",
    "    logs = []\n",
    "    for dir in dirs:\n",
    "        if dir[:5] == \"logs-\":\n",
    "            logs.append(dir)\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}/{experiment_name}/{logs[-1]}_combined\")\n",
    "    for dir in logs:\n",
    "        reader = SummaryReader(f\"{log_dir}/{experiment_name}/{dir}\")\n",
    "        dataFrames.append(reader.scalars)\n",
    "\n",
    "    final_dt = dataFrames[0]\n",
    "    scallar_tags = final_dt.tag.unique().tolist()\n",
    "\n",
    "    text = (reader.text)[\"value\"].values[0]\n",
    "\n",
    "    writer.add_text(tag=text_tag, text_string=text)\n",
    "    for tag in scallar_tags:\n",
    "        tag_dt = final_dt[final_dt[\"tag\"] == tag]\n",
    "        last_entry = tag_dt.tail(1)[\"step\"].index.values[0]\n",
    "        for dt in dataFrames[1:]:\n",
    "            new_entries = dt[dt.tag == tag]\n",
    "            tag_dt = pd.concat([tag_dt.iloc[:last_entry],new_entries,tag_dt.iloc[last_entry:]]).reset_index(drop=True)\n",
    "            tag_dt = tag_dt.drop_duplicates(subset='step', keep='first')\n",
    "            tag_dt = tag_dt.sort_values(\"step\").reset_index(drop=True)\n",
    "        for idx in range(len(tag_dt)):\n",
    "            entry = tag_dt.iloc[idx]\n",
    "            writer.add_scalar(tag=entry[\"tag\"], scalar_value=entry[\"value\"], global_step=entry[\"step\"])\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "log_dir = \"/home/manos/Thesis/opendr/projects/python/perception/object_detection_2d/nanodet/temp\"\n",
    "\n",
    "experiments = [\"augmented_data\", \"augmented_data_with_augmentations\", \"norm_data_with_augmentations\"]\n",
    "for experiment_name in experiments:\n",
    "    combine_tensorboard_graphs(log_dir, experiment_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['logs-2023-07-03-21-39-50']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = \"/home/manos/Thesis/opendr/projects/python/perception/object_detection_2d/nanodet/temp\"\n",
    "experiment_name = \"logs-2023-07-03-21-39-50\"\n",
    "text_tag = \"config\"\n",
    "dataFrames = []\n",
    "dirs = [experiment_name]\n",
    "logs = []\n",
    "for dir in dirs:\n",
    "    if dir[:5] == \"logs-\":\n",
    "        logs.append(dir)\n",
    "# dirs\n",
    "logs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T10:34:40.874444633Z",
     "start_time": "2023-07-04T10:34:40.871590729Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=f\"{log_dir}/{logs[-1]}_combined\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T10:36:31.551521954Z",
     "start_time": "2023-07-04T10:36:31.527612044Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manos/Thesis/opendr/projects/python/perception/object_detection_2d/nanodet/temp/logs-2023-07-03-21-39-50\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error: '/home/manos/Thesis/opendr/projects/python/perception/object_detection_2d/nanodet/temp/logs-2023-07-03-21-39-50'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mdir\u001B[39m \u001B[38;5;129;01min\u001B[39;00m logs:\n\u001B[0;32m----> 2\u001B[0m     reader \u001B[38;5;241m=\u001B[39m \u001B[43mSummaryReader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlog_dir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mdir\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     dataFrames\u001B[38;5;241m.\u001B[39mappend(reader\u001B[38;5;241m.\u001B[39mscalars)\n",
      "Cell \u001B[0;32mIn[14], line 155\u001B[0m, in \u001B[0;36mSummaryReader.__init__\u001B[0;34m(self, log_path, pivot, extra_columns, event_types)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    153\u001B[0m     \u001B[38;5;66;03m# Populate children\u001B[39;00m\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_path)\n\u001B[0;32m--> 155\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_path\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m    156\u001B[0m         filepath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_path, filename)\n\u001B[1;32m    157\u001B[0m         r \u001B[38;5;241m=\u001B[39m SummaryReader(filepath,\n\u001B[1;32m    158\u001B[0m                           pivot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pivot,\n\u001B[1;32m    159\u001B[0m                           extra_columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extra_columns,\n\u001B[1;32m    160\u001B[0m                           event_types\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_types)\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 5] Input/output error: '/home/manos/Thesis/opendr/projects/python/perception/object_detection_2d/nanodet/temp/logs-2023-07-03-21-39-50'"
     ]
    }
   ],
   "source": [
    "for dir in logs:\n",
    "    reader = SummaryReader(f\"{log_dir}/{dir}\")\n",
    "    dataFrames.append(reader.scalars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T10:39:19.987689190Z",
     "start_time": "2023-07-04T10:39:19.922398459Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "final_dt = dataFrames[0]\n",
    "scallar_tags = final_dt.tag.unique().tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mdir\u001B[39m[:\u001B[38;5;241m5\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogs-\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      9\u001B[0m         logs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mdir\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter(log_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlog_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexperiment_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlogs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_combined\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mdir\u001B[39m \u001B[38;5;129;01min\u001B[39;00m logs:\n\u001B[1;32m     12\u001B[0m     reader \u001B[38;5;241m=\u001B[39m SummaryReader(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlog_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexperiment_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mdir\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "text = (reader.text)[\"value\"].values[0]\n",
    "\n",
    "writer.add_text(tag=text_tag, text_string=text)\n",
    "for tag in scallar_tags:\n",
    "    tag_dt = final_dt[final_dt[\"tag\"] == tag]\n",
    "    last_entry = tag_dt.tail(1)[\"step\"].index.values[0]\n",
    "    for dt in dataFrames[1:]:\n",
    "        new_entries = dt[dt.tag == tag]\n",
    "        tag_dt = pd.concat([tag_dt.iloc[:last_entry],new_entries,tag_dt.iloc[last_entry:]]).reset_index(drop=True)\n",
    "        tag_dt = tag_dt.drop_duplicates(subset='step', keep='first')\n",
    "        tag_dt = tag_dt.sort_values(\"step\").reset_index(drop=True)\n",
    "    for idx in range(len(tag_dt)):\n",
    "        entry = tag_dt.iloc[idx]\n",
    "        writer.add_scalar(tag=entry[\"tag\"], scalar_value=entry[\"value\"], global_step=entry[\"step\"])\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T10:33:13.618304391Z",
     "start_time": "2023-07-04T10:33:13.531694187Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Provides a `SummaryReader` class that will read all tensorboard events and\n",
    "summaries in a directory contains multiple event files, or a single event file.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=C0302\n",
    "import copy\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from types import ModuleType\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple, Union, cast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing.event_accumulator import (\n",
    "    AUDIO, COMPRESSED_HISTOGRAMS, HISTOGRAMS, IMAGES, SCALARS,\n",
    "    STORE_EVERYTHING_SIZE_GUIDANCE, TENSORS, AudioEvent, EventAccumulator,\n",
    "    HistogramEvent, ImageEvent, ScalarEvent, TensorEvent)\n",
    "from tensorboard.plugins.hparams.plugin_data_pb2 import HParamsPluginData\n",
    "try:\n",
    "    import tensorflow\n",
    "except ImportError:\n",
    "    tensorflow = None\n",
    "\n",
    "# pylint: disable=W0105\n",
    "\"\"\"\n",
    "from tensorboard.backend.event_processing.event_accumulator import (\n",
    "    AUDIO, COMPRESSED_HISTOGRAMS, GRAPH, HISTOGRAMS, IMAGES, META_GRAPH,\n",
    "    RUN_METADATA, SCALARS, STORE_EVERYTHING_SIZE_GUIDANCE, TENSORS, AudioEvent,\n",
    "    CompressedHistogramEvent, EventAccumulator, HistogramEvent, ImageEvent,\n",
    "    ScalarEvent, TensorEvent)\n",
    "\"\"\"\n",
    "\n",
    "HPARAMS = 'hparams'\n",
    "TEXT = 'text'\n",
    "PLUGIN_TAGS = {HPARAMS, TEXT}\n",
    "PLUGIN_RAW_TAGS = {\n",
    "    \"hparams/exp\": \"_hparams_/experiment\",\n",
    "    \"hparams/ssi\": \"_hparams_/session_start_info\",\n",
    "    \"hparams/sei\": \"_hparams_/session_end_info\",\n",
    "    TEXT: \"/text_summary\",\n",
    "}\n",
    "\n",
    "MINIMUM_SIZE_GUIDANCE = {\n",
    "    COMPRESSED_HISTOGRAMS: 1,\n",
    "    IMAGES: 1,\n",
    "    AUDIO: 1,\n",
    "    SCALARS: 1,\n",
    "    HISTOGRAMS: 1,\n",
    "    TENSORS: 1,\n",
    "}\n",
    "\n",
    "ALL_EVENT_TYPES = {SCALARS, TENSORS, HISTOGRAMS, IMAGES, AUDIO, HPARAMS, TEXT}\n",
    "REDUCED_EVENT_TYPES = {SCALARS, HISTOGRAMS, HPARAMS}\n",
    "ALL_EXTRA_COLUMNS = {'dir_name', 'file_name', 'wall_time', 'min', 'max', 'num',\n",
    "                     'sum', 'sum_squares', 'width', 'height', 'content_type',\n",
    "                     'length_frames', 'sample_rate'}\n",
    "\n",
    "\n",
    "# pylint: disable=R0904\n",
    "class SummaryReader():\n",
    "    \"\"\"\n",
    "    Creates a `SummaryReader` that reads all tensorboard events and summaries\n",
    "    stored in a event file or a directory containing multiple event files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, log_path: str, *, pivot=False, extra_columns=None,\n",
    "                 event_types=None):\n",
    "        \"\"\"The constructor of SummaryReader. Columns contains `step`, `tag`, \\\n",
    "           and `value` by default.\n",
    "\n",
    "        :param log_path: Load directory location, or load file location.\n",
    "        :type log_path: str\n",
    "\n",
    "        :param pivot: Returns long format DataFrame by default, \\\n",
    "                returns wide format DataFrame if set to True. If there are \\\n",
    "                multiple values per step with the same tag, the values are \\\n",
    "                merged into a list.\n",
    "        :type pivot: bool\n",
    "\n",
    "        :param extra_columns: Specifies extra columns, defaults to `None`.\n",
    "\n",
    "                - dir_name:  add a column that contains the relative \\\n",
    "                             directory path.\n",
    "                - file_name: add a column that contains the relative \\\n",
    "                             event file path.\n",
    "                - wall_time: add a column that stores the event timestamp.\n",
    "                - min (histogram): the min value in the histogram.\n",
    "                - max (histogram): the max value in the histogram.\n",
    "                - num (histogram): the number of values in the histogram.\n",
    "                - sum (histogram): the sum of all values in the histogram.\n",
    "                - sum_squares (histogram): the sum of squares for all values \\\n",
    "                                           in the histogram.\n",
    "                - width (image): the width of the image.\n",
    "                - height (image): the height of the image.\n",
    "                - content_type (audio): the content type of the audio.\n",
    "                - length_frames (audio): the length of the audio.\n",
    "                - sample_rate (audio): the sampling rate of the audio.\n",
    "        :type extra_columns: Set[{'dir_name', 'file_name', 'wall_time', \\\n",
    "                'min', 'max', 'num', 'sum', 'sum_squares', 'width', 'height', \\\n",
    "                'content_type', 'length_frames', 'sample_rate'}]\n",
    "\n",
    "        :param event_types: Specifies the event types to parse, \\\n",
    "            defaults to all event types.\n",
    "        :type event_types: Set[{'scalars', 'tensors', 'histograms', 'images', \\\n",
    "            'audio', 'hparams', 'text'}]\n",
    "        \"\"\"\n",
    "        self._log_path: str = log_path\n",
    "        \"\"\"Load directory location, or load file location.\"\"\"\n",
    "        self._extra_columns: Set[str] = (extra_columns or set()).copy()\n",
    "        \"\"\"Specifies additional required columns.\"\"\"\n",
    "        if not isinstance(self._extra_columns, set):\n",
    "            raise ValueError(f\"`columns` should be a {set} instead of \\\n",
    "                              {str(type(self._extra_columns))}\")\n",
    "        diff = self._extra_columns - ALL_EXTRA_COLUMNS\n",
    "        if len(diff) > 0:\n",
    "            raise KeyError(f\"Invalid columns entries: {diff}\")\n",
    "        self._pivot: bool = pivot\n",
    "        \"\"\"Determines whether the DataFrame is stored in wide format.\"\"\"\n",
    "        self._event_types: Set[str] = (event_types or ALL_EVENT_TYPES).copy()\n",
    "        \"\"\"Specifies the event types to parse.\"\"\"\n",
    "        if tensorflow is None:\n",
    "            self._event_types = (event_types or REDUCED_EVENT_TYPES).copy()\n",
    "        if not isinstance(self._event_types, set):\n",
    "            raise ValueError(f\"`event_types` should be a {set} instead of \\\n",
    "                              {str(type(self._event_types))}\")\n",
    "        diff = self._event_types - ALL_EVENT_TYPES\n",
    "        if len(diff) > 0:\n",
    "            raise KeyError(f\"Invalid event types: {diff}\")\n",
    "        self._children: Dict[str, 'SummaryReader'] = {}\n",
    "        \"\"\"Holds a list of references to the `SummaryReader` children.\"\"\"\n",
    "\n",
    "        self._tags: Optional[Dict[str, List[str]]] = None\n",
    "        \"\"\"Stores a dictionary contatining a list of parsed tag names for each\n",
    "        event type.\"\"\"\n",
    "        self._events: Dict[str, pd.DataFrame] = self._make_empty_dict(None)\n",
    "        \"\"\"Stores a `pandas.DataFrame` containing all events.\"\"\"\n",
    "\n",
    "        if not os.path.exists(self.log_path):\n",
    "            raise ValueError(f\"File or directory not found: {self.log_path}\")\n",
    "        if os.path.isfile(self.log_path):\n",
    "            # Note: tensorflow.python.summary.summary_iterator is less\n",
    "            #       straightforward, so we use EventAccumulator instead.\n",
    "            size_guidance = MINIMUM_SIZE_GUIDANCE.copy()\n",
    "            for e in self._event_types:\n",
    "                size_guidance[e] = 0  # store everything\n",
    "            event_acc = EventAccumulator(self.log_path, size_guidance)\n",
    "            event_acc.Reload()\n",
    "            self._tags = self._make_empty_dict([])\n",
    "            for e in self._event_types:\n",
    "                self._parse_events(e, event_acc=event_acc)\n",
    "        else:\n",
    "            # Populate children\n",
    "            print(self.log_path)\n",
    "            for filename in sorted(os.listdir(self.log_path)):\n",
    "                filepath = os.path.join(self.log_path, filename)\n",
    "                r = SummaryReader(filepath,\n",
    "                                  pivot=self._pivot,\n",
    "                                  extra_columns=self._extra_columns,\n",
    "                                  event_types=self._event_types)\n",
    "                self._children[filename] = r\n",
    "\n",
    "    @property\n",
    "    def log_path(self) -> str:\n",
    "        \"\"\"Load directory location, or load file location.\n",
    "\n",
    "        :return: A directory path or file path.\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        return self._log_path\n",
    "\n",
    "    @property\n",
    "    def tags(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Returns a dictionary contatining a list of parsed tag names for each\n",
    "        event type.\n",
    "\n",
    "        :return: A `{eventType: ['list', 'of', 'tags']}` dictionary.\n",
    "        :rtype: Dict[str, List[str]]\n",
    "        \"\"\"\n",
    "        return cast(Dict[str, List[str]], self.get_tags())\n",
    "\n",
    "    def get_tags(self, event_type: str = None) -> \\\n",
    "            Union[List[str], Dict[str, List[str]]]:\n",
    "        \"\"\"Returns a list of tag names for the specified event type. If\n",
    "        `event_type` is None, return a dictionary containing a list of tag\n",
    "        names for each event type.\n",
    "\n",
    "        :param event_type: the event type to retrieve, None means return all, \\\n",
    "        defaults to None.\n",
    "        :type event_type: {None, 'scalars', 'tensors', 'histograms', \\\n",
    "            'images', 'audio', 'hparams', 'text'}, optional\n",
    "        :raises ValueError: if `event_type` is unknown.\n",
    "        :return: A `['list', 'of', 'tags']` list, or a \\\n",
    "            `{eventType: ['list', 'of', 'tags']}` dictionary.\n",
    "        :rtype: List[str] | Dict[str, List[str]]\n",
    "        \"\"\"\n",
    "        if event_type not in {None}.union(ALL_EVENT_TYPES):\n",
    "            raise ValueError(f\"Unknown event_type: {event_type}\")\n",
    "        if self._tags is not None:\n",
    "            # Leaf node returns directly\n",
    "            if event_type is not None:\n",
    "                return self._tags[event_type].copy()\n",
    "            return copy.deepcopy(self._tags)\n",
    "        # Non-leaf node collects children's tags then return\n",
    "        tags = self._make_empty_dict([])\n",
    "        if event_type is not None:\n",
    "            # Only collect the specified event type\n",
    "            tags = {event_type: tags[event_type]}\n",
    "        for t in list(tags.keys()):\n",
    "            for c in self.children.values():\n",
    "                # Collect children's tags\n",
    "                tags[t] += c.get_tags(t)\n",
    "            # Deduplicate same tag names\n",
    "            tags[t] = list(dict.fromkeys(tags[t]))\n",
    "        if event_type is not None:\n",
    "            return tags[event_type]\n",
    "        return tags\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_values(s: pd.Series):\n",
    "        \"\"\"Merge multiple values. Ignore NaNs, concat others.\"\"\"\n",
    "        # Does not support python3.6 since DataFrame does not fully support\n",
    "        # `np.ndarray` as an element in cell. See the following:\n",
    "        # lib/python3.6/site-packages/pandas/core/groupby/generic.py:482\n",
    "        # Python 3.6 EOL: 2021-12-23 (https://www.python.org/downloads/)\n",
    "        try:\n",
    "            assert isinstance(s, pd.Series)\n",
    "            if len(s) == 1:\n",
    "                return s  # Return directly if no merging is needed\n",
    "            lst = []\n",
    "            for xx in s:\n",
    "                if isinstance(xx, list):\n",
    "                    lst.extend(xx)\n",
    "                elif np.isscalar(xx):\n",
    "                    if not pd.isnull(xx):\n",
    "                        lst.append(xx)\n",
    "                else:\n",
    "                    lst.append(xx)\n",
    "            if len(lst) == 0:\n",
    "                return np.nan\n",
    "            if len(lst) == 1:\n",
    "                return lst[0]\n",
    "            return lst\n",
    "        except Exception as error:\n",
    "            # Pandas ignores some errors by default\n",
    "            raise ValueError from error\n",
    "\n",
    "    def get_events(self, event_type: str) -> pd.DataFrame:\n",
    "        \"\"\"Construct a `pandas.DataFrame` that stores all `event_type` events \\\n",
    "        under `log_path`. Some processing is performed when evaluating this \\\n",
    "        property. Therefore you may want to store the results and reuse it \\\n",
    "        for better performance.\n",
    "\n",
    "        :type event_type: {'scalars', 'tensors', 'histograms', 'images', \\\n",
    "            'audio', 'hparams', 'text'}.\n",
    "        :raises ValueError: if `event_type` is unknown.\n",
    "        :return: A `DataFrame` storing all `event_type` events.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        if event_type not in ALL_EVENT_TYPES:\n",
    "            raise ValueError(f\"Unknown event_type: {event_type}\")\n",
    "        if event_type not in REDUCED_EVENT_TYPES and tensorflow is None:\n",
    "            self._get_tensorflow()  # raise error\n",
    "        if event_type not in self._event_types:\n",
    "            raise ValueError(f\"event_type is ignored by user: {event_type}\")\n",
    "        group_columns: List[Any] = list(filter(\n",
    "            lambda x: x in self._extra_columns, ['dir_name', 'file_name']))\n",
    "        dfs = []\n",
    "        if os.path.isfile(self.log_path):\n",
    "            # Leaf node appends events directly\n",
    "            dfs.append(self._events[event_type])\n",
    "        else:\n",
    "            # Non-leaf node collects children's events\n",
    "            for child in self._children.values():\n",
    "                df = child.get_events(event_type)\n",
    "                # iteratively prepend dir_name\n",
    "                if 'dir_name' in df and os.path.isdir(child.log_path):\n",
    "                    dir_name = os.path.basename(child.log_path)\n",
    "                    df_cond = (df['dir_name'] == '')\n",
    "                    df.loc[df_cond, 'dir_name'] = dir_name\n",
    "                    df.loc[~df_cond, 'dir_name'] = \\\n",
    "                        dir_name + '/' + df.loc[~df_cond, 'dir_name']\n",
    "                dfs.append(df)\n",
    "        dfs = list(filter(lambda x: x is not None, dfs))\n",
    "        if len(dfs) == 0:\n",
    "            return pd.DataFrame()\n",
    "        df_stacked = pd.concat(dfs, ignore_index=True)\n",
    "        if df_stacked.empty:\n",
    "            return pd.DataFrame()\n",
    "        if not self._pivot:\n",
    "            group_columns += ['tag']\n",
    "\n",
    "        group_columns += ['step']\n",
    "        # Don't sort by wall_time, since there is only a single value per step\n",
    "        # in most cases\n",
    "        group_columns = list(filter(\n",
    "            lambda x: x in df_stacked.columns, group_columns))\n",
    "        df_stacked.sort_values(group_columns, ignore_index=True, inplace=True)\n",
    "        if not self._pivot:\n",
    "            return df_stacked\n",
    "        if len(group_columns) == 0:\n",
    "            # merge all rows\n",
    "            group_columns = [True] * len(df_stacked)\n",
    "        # Merge if there are multiple values per step with the same tag\n",
    "        grouped = df_stacked.groupby(group_columns, sort=False)\n",
    "        df = grouped.aggregate(self._merge_values)\n",
    "        df.reset_index(inplace=True)\n",
    "        # Reorder columns\n",
    "        middle_columns = list(filter(\n",
    "            lambda x: x not in\n",
    "            ['step', 'wall_time', 'dir_name', 'file_name'],\n",
    "            df_stacked.columns))\n",
    "        middle_columns = sorted(middle_columns)  # sort tags\n",
    "        columns = ['step'] + middle_columns + \\\n",
    "            ['wall_time', 'dir_name', 'file_name']\n",
    "        columns = list(filter(lambda x: x in df_stacked.columns, columns))\n",
    "        return df[columns]  # reorder since values are merged\n",
    "\n",
    "    @property\n",
    "    def scalars(self) -> pd.DataFrame:\n",
    "        \"\"\"Construct a `pandas.DataFrame` that stores all scalar events under \\\n",
    "        `log_path`. Some processing is performed when evaluating this \\\n",
    "        property. Therefore you may want to store the results and reuse it \\\n",
    "        for better performance.\n",
    "\n",
    "        :return: A `DataFrame` storing all scalar events.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self.get_events(SCALARS)\n",
    "\n",
    "    @property\n",
    "    def tensors(self) -> pd.DataFrame:\n",
    "        \"\"\"Construct a `pandas.DataFrame` that stores all tensor events under \\\n",
    "        `log_path`. Some processing is performed when evaluating this \\\n",
    "        property. Therefore you may want to store the results and reuse it \\\n",
    "        for better performance.\n",
    "\n",
    "        :return: A `DataFrame` storing all tensor events.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self.get_events(TENSORS)\n",
    "\n",
    "    @property\n",
    "    def histograms(self) -> pd.DataFrame:\n",
    "        \"\"\"Construct a `pandas.DataFrame` that stores all histograms events\n",
    "        under `log_path`. Some processing is performed when evaluating this \\\n",
    "        property. Therefore you may want to store the results and reuse it \\\n",
    "        for better performance.\n",
    "\n",
    "        :return: A `DataFrame` storing all histograms events.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self.get_events(HISTOGRAMS)\n",
    "\n",
    "    @property\n",
    "    def images(self) -> pd.DataFrame:\n",
    "        \"\"\"Construct a `pandas.DataFrame` that stores all images events under \\\n",
    "        log_path`. Some processing is performed when evaluating this \\\n",
    "        property. Therefore you may want to store the results and reuse it \\\n",
    "        for better performance.\n",
    "\n",
    "        :return: A `DataFrame` storing all images events.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self.get_events(IMAGES)\n",
    "\n",
    "    @property\n",
    "    def audio(self) -> pd.DataFrame:\n",
    "        \"\"\"Construct a `pandas.DataFrame` that stores all audio events under \\\n",
    "        `log_path`. Some processing is performed when evaluating this \\\n",
    "        property. Therefore you may want to store the results and reuse it \\\n",
    "        for better performance.\n",
    "\n",
    "        :return: A `DataFrame` storing all audio events.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self.get_events(AUDIO)\n",
    "\n",
    "    @property\n",
    "    def hparams(self) -> pd.DataFrame:\n",
    "        \"\"\"Construct a `pandas.DataFrame` that stores all hparams events\n",
    "        under `log_path`. Some processing is performed when evaluating this \\\n",
    "        property. Therefore you may want to store the results and reuse it \\\n",
    "        for better performance.\n",
    "\n",
    "        :return: A `DataFrame` storing all hparams events.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self.get_events(HPARAMS)\n",
    "\n",
    "    @property\n",
    "    def text(self) -> pd.DataFrame:\n",
    "        \"\"\"Construct a `pandas.DataFrame` that stores all text events\n",
    "        under `log_path`. Some processing is performed when evaluating this \\\n",
    "        property. Therefore you may want to store the results and reuse it \\\n",
    "        for better performance.\n",
    "\n",
    "        :return: A `DataFrame` storing all hparams events.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self.get_events(TEXT)\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_to_histogram(tensor: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a tensor to histogram dictionary.\n",
    "\n",
    "        :param tensor: A `[['left edge', 'right edge', 'count']]` \\\n",
    "        list. The range of the bucket is [lower, upper)\n",
    "        :type tensor: np.ndarray\n",
    "        :return: A `{hist_data_name: hist_data}` dictionary.\n",
    "        :rtype: Dict[str, Any]\n",
    "        \"\"\"\n",
    "        limits = [tensor[0][0]] + list(map(lambda x: x[1], tensor))\n",
    "        counts = [0] + list(map(lambda x: x[2], tensor))\n",
    "        assert len(limits) == len(tensor) + 1\n",
    "        assert len(limits) == len(counts)\n",
    "        d = {\n",
    "            'limits': np.array(limits),\n",
    "            'counts': np.array(counts),\n",
    "            'min': limits[0],\n",
    "            'max': limits[-1],\n",
    "            'num': np.sum(counts),\n",
    "            'sum': np.nan,\n",
    "            'sum_squares': np.nan,\n",
    "        }\n",
    "        return d\n",
    "\n",
    "    @staticmethod\n",
    "    def buckets_to_histogram_dict(lst: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a list of buckets to histogram dictionary. \\\n",
    "        (deprecated, use `tensor_to_histogram` instead)\n",
    "\n",
    "        :param lst: A `[['bucket lower', 'bucket upper', 'bucket count']]` \\\n",
    "        list. The range of the bucket is [lower, upper)\n",
    "        :type lst: np.ndarray\n",
    "        :return: A `{hist_data_name: hist_data}` dictionary.\n",
    "        :rtype: Dict[str, Any]\n",
    "        \"\"\"\n",
    "        return SummaryReader.tensor_to_histogram(lst)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_tensorflow() -> ModuleType:\n",
    "        if tensorflow is not None:\n",
    "            return tensorflow\n",
    "        raise ModuleNotFoundError(\"No module named 'tensorflow'. \" +\n",
    "              \"Please install 'tensorflow' or 'tensorflow-cpu'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_to_image(tensor: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a tensor to image dictionary.\n",
    "\n",
    "        :param tensor: A `['width', 'height', 'encoded image', ...]` list.\n",
    "        :type tensor: np.ndarray\n",
    "        :return: A `{image_data_name: image_data}` dictionary.\n",
    "        :rtype: Dict[str, Any]\n",
    "        \"\"\"\n",
    "        # pylint: disable=C0103\n",
    "        tf = SummaryReader._get_tensorflow()\n",
    "        lst = list(map(tf.image.decode_image, tensor[2:]))\n",
    "        lst = list(map(lambda x: x.numpy(), lst))\n",
    "        image = np.stack(lst, axis=0)\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.squeeze(axis=0)\n",
    "        d = {\n",
    "            'image': image,\n",
    "            'width': int(tensor[0]),\n",
    "            'height': int(tensor[1]),\n",
    "        }\n",
    "        return d\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_to_audio(tensor: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a audio to audio dictionary.\n",
    "\n",
    "        :param tensor: A `[['encoded audio', b''], ...]` list.\n",
    "        :type tensor: np.ndarray\n",
    "        :return: A `{audio_data_name: audio_data}` dictionary.\n",
    "        :rtype: Dict[str, Any]\n",
    "        \"\"\"\n",
    "        # pylint: disable=C0103\n",
    "        tf = SummaryReader._get_tensorflow()\n",
    "        assert tensor[:, 1].tolist() == [b''] * tensor.shape[0]\n",
    "        lst = list(map(tf.audio.decode_wav, tensor[:, 0]))\n",
    "        audio_lst = list(map(lambda x: x[0].numpy(), lst))\n",
    "        sample_rate_lst = list(map(lambda x: x[1].numpy(), lst))\n",
    "        audio = np.stack(audio_lst, axis=0)\n",
    "        sample_rate = np.stack(sample_rate_lst, axis=0)\n",
    "        length = audio.shape[1]\n",
    "        if audio.shape[0] == 1:\n",
    "            audio = audio.squeeze(axis=0)\n",
    "            sample_rate = sample_rate.squeeze(axis=0)\n",
    "        d = {\n",
    "            'audio': audio,\n",
    "            'content_type': 'audio/wav',\n",
    "            'length_frames': length,\n",
    "            'sample_rate': sample_rate,\n",
    "        }\n",
    "        return d\n",
    "\n",
    "    @staticmethod\n",
    "    def histogram_to_pdf(counts: np.ndarray, limits: np.ndarray,\n",
    "                         x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Given an array of `x` (values), returns the pair (`c`, `y`), which\n",
    "        are the corresponding `c` (bucket center) and the linear interpolation\n",
    "        of its `y` (probability density in bucket), given the bucket counts\n",
    "        and limits.\n",
    "\n",
    "        :param counts: The number of values inside the buckets. The first \\\n",
    "            value must be zero.\n",
    "        :type counts: np.ndarray\n",
    "        :param limits: The (right) edges of the buckets. The first value is \\\n",
    "            the left edge of the first bucket.\n",
    "        :type limits: np.ndarray\n",
    "        :param x: The input values of x.\n",
    "        :type x: np.ndarray\n",
    "        :return: The tuple containing the bucket center and the \\\n",
    "            probability density of the bucket.\n",
    "        :rtype: Tuple[np.ndarray, np.ndarray]\n",
    "        \"\"\"\n",
    "        y = SummaryReader.histogram_to_cdf(counts, limits, x)\n",
    "        center = (x[1:]+x[:-1])/2\n",
    "        density = (y[1:]-y[:-1])/(x[1:]-x[:-1])\n",
    "        return center, density\n",
    "\n",
    "    @staticmethod\n",
    "    def histogram_to_cdf(counts: np.ndarray, limits: np.ndarray,\n",
    "                         x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Given an array of `x` (values), returns the linear interpolation of\n",
    "        its corresponding `y` (cumulative probability), given the bucket counts\n",
    "        and limits.\n",
    "\n",
    "        :param counts: The number of values inside the buckets. The first \\\n",
    "            value must be zero.\n",
    "        :type counts: np.ndarray\n",
    "        :param limits: The (right) edges of the buckets. The first value is \\\n",
    "            the left edge of the first bucket.\n",
    "        :type limits: np.ndarray\n",
    "        :param x: The input values of x coordinates.\n",
    "        :type x: np.ndarray\n",
    "        :return: `y`, the cumulative probability at values `x`.\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        assert len(counts) == len(limits)\n",
    "        counts = np.array(counts)\n",
    "        limits = np.array(limits)\n",
    "        n = np.sum(counts)\n",
    "        x = np.array(x)\n",
    "        # x must be increasing\n",
    "        assert np.all(np.diff(x) > 0)\n",
    "\n",
    "        cumsum = np.cumsum(counts)\n",
    "        assert len(cumsum) == len(limits)\n",
    "\n",
    "        y: List[int] = []\n",
    "        # Calculate y[i], where x[i] <= limits[0]\n",
    "        i = 0\n",
    "        while i < len(x) and x[i] <= limits[0]:\n",
    "            y.append(0)\n",
    "            i += 1\n",
    "        # Calculate y[i], where limits[0] < x[i] <= limits[-1]\n",
    "        idx = 0\n",
    "        while i < len(x) and idx + 1 < len(limits):\n",
    "            if limits[idx+1] < x[i]:\n",
    "                idx += 1\n",
    "                continue\n",
    "            lower = limits[idx]\n",
    "            upper = limits[idx+1]\n",
    "            assert lower < x[i] and x[i] <= upper\n",
    "            assert (x[i] - lower) > 0\n",
    "            interp = (cumsum[idx] * (upper - x[i]) +\n",
    "                      cumsum[idx+1] * (x[i] - lower))\n",
    "            interp /= (upper - lower)\n",
    "            y.append(interp)\n",
    "            i += 1\n",
    "        # Calculate y[i], where limits[-1] < x[i]\n",
    "        while i < len(x):\n",
    "            y.append(n)\n",
    "            i += 1\n",
    "        return np.array(y) / n\n",
    "\n",
    "    # pylint: disable=R0914\n",
    "    @staticmethod\n",
    "    def histogram_to_bins(counts: np.ndarray, limits: np.ndarray,\n",
    "                          lower_bound: float = None, upper_bound: float = None,\n",
    "                          n_bins: int = 30):\n",
    "        \"\"\"Returns the pair (`c`, `y`), which are the corresponding `c`\n",
    "        (bin center) and `y` (counts in bucket), given the bucket counts\n",
    "        and limits.\n",
    "\n",
    "        :param counts: The number of values inside the buckets. The first \\\n",
    "            value must be zero.\n",
    "        :type counts: np.ndarray\n",
    "        :param limits: The (right) edges of the buckets. The first value is \\\n",
    "            the left edge of the first bucket.\n",
    "        :type limits: np.ndarray\n",
    "        :param lower_bound: The left edge of the first bin.\n",
    "        :type lower_bound: float\n",
    "        :param upper_bound: The right edge of the last bin.\n",
    "        :type upper_bound: float\n",
    "        :param n_bins: The number of output bins.\n",
    "        :type n_bins: int\n",
    "        :return: The tuple containing the bin center and the counts in \\\n",
    "            each bucket.\n",
    "        :rtype: Tuple[np.ndarray, np.ndarray]\n",
    "        \"\"\"\n",
    "        # pylint: disable=C0301\n",
    "        # Ref: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/histogram/tf_histogram_dashboard/histogramCore.ts#L83 # noqa: E501\n",
    "        assert len(counts) == len(limits)\n",
    "        assert counts[0] == 0\n",
    "        if lower_bound is None or upper_bound is None:\n",
    "            lower_bound = upper_bound = 0\n",
    "        if upper_bound == lower_bound:\n",
    "            # If the output range is 0 width, use a default non 0 range for\n",
    "            # visualization purpose.\n",
    "            upper_bound = lower_bound * 1.1 + 1\n",
    "            lower_bound = lower_bound / 1.1 - 1\n",
    "        # Terminology note: `buckets` are the input to this function,\n",
    "        # while _bins_ are our output.\n",
    "        bin_width = (upper_bound - lower_bound) / n_bins\n",
    "        bucket_idx = 1\n",
    "        centers = []\n",
    "        bins = []\n",
    "        for i in range(n_bins):\n",
    "            bin_left = lower_bound + i * bin_width\n",
    "            bin_right = bin_left + bin_width\n",
    "            # Take the count of each existing bucket, multiply it by the\n",
    "            # proportion of overlap with the new bin, then sum and store as the\n",
    "            # count for the new bin. If no overlap, will add to zero; if 100%\n",
    "            # overlap, will include the full count into new bin.\n",
    "            bin_y = 0\n",
    "            while bucket_idx < len(counts):\n",
    "                # Clip the right edge because right-most edge can be\n",
    "                # infinite-sized.\n",
    "                bucket_right = min(upper_bound, limits[bucket_idx])\n",
    "                bucket_left = max(lower_bound, limits[bucket_idx-1])\n",
    "                bucket_width = bucket_right - bucket_left\n",
    "                if bucket_width > 0:\n",
    "                    intersect = min(bucket_right, bin_right) \\\n",
    "                        - max(bucket_left, bin_left)\n",
    "                    count = (intersect / (bucket_right - bucket_left)) \\\n",
    "                        * counts[bucket_idx]\n",
    "                    bin_y += count if intersect > 0 else 0\n",
    "                else:\n",
    "                    is_final_bin = (bin_right >= upper_bound)\n",
    "                    single_value_overlap = \\\n",
    "                        (bin_left <= bucket_left and\n",
    "                         ((bucket_right <= bin_right) if is_final_bin else\n",
    "                          (bucket_right < bin_right)))\n",
    "                    bin_y += counts[bucket_idx] if single_value_overlap else 0\n",
    "                if bucket_right > bin_right:\n",
    "                    break\n",
    "                bucket_idx += 1\n",
    "            centers.append(bin_left + bin_width / 2)\n",
    "            bins.append(bin_y)\n",
    "        return centers, bins\n",
    "\n",
    "    def _get_scalar_cols(self, tag_to_events: Dict[str, ScalarEvent]) -> \\\n",
    "            Dict[str, List[Any]]:\n",
    "        \"\"\"Return a dict of lists based on the tags and ScalarEvents.\"\"\"\n",
    "        cols = self._get_default_cols(tag_to_events)\n",
    "        idx = 0\n",
    "        for tag, events in tag_to_events.items():\n",
    "            for e in events:\n",
    "                cols['step'][idx] = e.step\n",
    "                if self._pivot:\n",
    "                    cols[tag][idx] = e.value\n",
    "                else:\n",
    "                    cols['tag'][idx] = tag\n",
    "                    cols['value'][idx] = e.value\n",
    "                idx += 1\n",
    "        return cols\n",
    "\n",
    "    def _get_tensor_cols(self, tag_to_events: Dict[str, TensorEvent]) -> \\\n",
    "            Dict[str, List[Any]]:\n",
    "        \"\"\"Return a dict of lists based on the tags and TensorEvents.\"\"\"\n",
    "        cols = self._get_default_cols(tag_to_events)\n",
    "        if len(tag_to_events) == 0:\n",
    "            return cols\n",
    "        # pylint: disable=C0103\n",
    "        tf = SummaryReader._get_tensorflow()\n",
    "        idx = 0\n",
    "        for tag, events in tag_to_events.items():\n",
    "            for e in events:\n",
    "                value = tf.make_ndarray(e.tensor_proto)\n",
    "                if value.shape == ():\n",
    "                    # Tensorflow histogram may have more than one items\n",
    "                    value = value.item()\n",
    "                cols['step'][idx] = e.step\n",
    "                if self._pivot:\n",
    "                    cols[tag][idx] = value\n",
    "                else:\n",
    "                    cols['tag'][idx] = tag\n",
    "                    cols['value'][idx] = value\n",
    "                idx += 1\n",
    "        return cols\n",
    "\n",
    "    def _get_histogram_cols(self, tag_to_events: Dict[str, HistogramEvent]) \\\n",
    "            -> Dict[str, List[Any]]:\n",
    "        \"\"\"Return a dict of lists based on the tags and HistogramEvent.\"\"\"\n",
    "        cols = self._get_default_cols(tag_to_events)\n",
    "        idx = 0\n",
    "        for tag, events in tag_to_events.items():\n",
    "            for e in events:\n",
    "                hv = e.histogram_value\n",
    "                limits = np.array(hv.bucket_limit, dtype=np.float64)\n",
    "                counts = np.array(hv.bucket, dtype=np.float64)\n",
    "                columns = {\n",
    "                    'counts': counts,\n",
    "                    'limits': limits,\n",
    "                    'max': hv.max,\n",
    "                    'min': hv.min,\n",
    "                    'num': hv.num,\n",
    "                    'sum': hv.sum,\n",
    "                    'sum_squares': hv.sum_squares,\n",
    "                }\n",
    "                # assert list(columns.keys()) == list(sorted(columns.keys()))\n",
    "                cols['step'][idx] = e.step\n",
    "                if not self._pivot:\n",
    "                    cols['tag'][idx] = tag\n",
    "                lst = list(self._extra_columns) + ['limits', 'counts']\n",
    "                for k, v in columns.items():\n",
    "                    if k in lst:\n",
    "                        key = k if not self._pivot else tag + '/' + k\n",
    "                        cols[key][idx] = v\n",
    "                idx += 1\n",
    "        return cols\n",
    "\n",
    "    def _get_image_cols(self, tag_to_events: Dict[str, ImageEvent]) -> \\\n",
    "            Dict[str, List[Any]]:\n",
    "        \"\"\"Return a dict of lists based on the tags and ImageEvent.\"\"\"\n",
    "        cols = self._get_default_cols(tag_to_events)\n",
    "        if len(tag_to_events) == 0:\n",
    "            return cols\n",
    "        # pylint: disable=C0103\n",
    "        tf = SummaryReader._get_tensorflow()\n",
    "        idx = 0\n",
    "        for tag, events in tag_to_events.items():\n",
    "            for e in events:\n",
    "                value = tf.image.decode_image(e.encoded_image_string).numpy()\n",
    "                columns = {\n",
    "                    'height': e.height,\n",
    "                    'width': e.width,\n",
    "                }\n",
    "                # assert list(columns.keys()) == list(sorted(columns.keys()))\n",
    "                cols['step'][idx] = e.step\n",
    "                if self._pivot:\n",
    "                    cols[tag][idx] = value\n",
    "                else:\n",
    "                    cols['tag'][idx] = tag\n",
    "                    cols['value'][idx] = value\n",
    "                for k, v in columns.items():\n",
    "                    if k in self._extra_columns:\n",
    "                        key = k if not self._pivot else tag + '/' + k\n",
    "                        cols[key][idx] = v\n",
    "                idx += 1\n",
    "        return cols\n",
    "\n",
    "    def _get_audio_cols(self, tag_to_events: Dict[str, AudioEvent]) -> \\\n",
    "            Dict[str, List[Any]]:\n",
    "        \"\"\"Return a dict of lists based on the tags and AudioEvent.\"\"\"\n",
    "        cols = self._get_default_cols(tag_to_events)\n",
    "        if len(tag_to_events) == 0:\n",
    "            return cols\n",
    "        # pylint: disable=C0103\n",
    "        tf = SummaryReader._get_tensorflow()\n",
    "        idx = 0\n",
    "        for tag, events in tag_to_events.items():\n",
    "            for e in events:\n",
    "                audio, _ = tf.audio.decode_wav(e.encoded_audio_string)\n",
    "                value = audio.numpy()\n",
    "                columns = {\n",
    "                    'content_type': e.content_type,\n",
    "                    'length_frames': e.length_frames,\n",
    "                    'sample_rate': e.sample_rate,\n",
    "                }\n",
    "                # assert list(columns.keys()) == list(sorted(columns.keys()))\n",
    "                cols['step'][idx] = e.step\n",
    "                if self._pivot:\n",
    "                    cols[tag][idx] = value\n",
    "                else:\n",
    "                    cols['tag'][idx] = tag\n",
    "                    cols['value'][idx] = value\n",
    "                for k, v in columns.items():\n",
    "                    if k in self._extra_columns:\n",
    "                        key = k if not self._pivot else tag + '/' + k\n",
    "                        cols[key][idx] = v\n",
    "                idx += 1\n",
    "        return cols\n",
    "\n",
    "    def _get_hparam_cols(self, tag_to_events: Dict[str, Any]) -> \\\n",
    "            Dict[str, List[Any]]:\n",
    "        \"\"\"Return a dict of lists based on the tags and HParamsPluginData.\"\"\"\n",
    "        cols = self._get_default_cols(tag_to_events, wall_time=False)\n",
    "        idx = 0\n",
    "        for tag, value in tag_to_events.items():\n",
    "            if self._pivot:\n",
    "                cols[tag][idx] = value[0]\n",
    "            else:\n",
    "                cols['tag'][idx] = tag\n",
    "                cols['value'][idx] = value[0]\n",
    "            idx += 1\n",
    "        return cols\n",
    "\n",
    "    def _get_text_cols(self, tag_to_events: Dict[str, TensorEvent]) -> \\\n",
    "            Dict[str, List[Any]]:\n",
    "        \"\"\"Return a dict of lists based on the tags and TensorEvent.\"\"\"\n",
    "        cols = self._get_default_cols(tag_to_events)\n",
    "        if len(tag_to_events) == 0:\n",
    "            return cols\n",
    "        # pylint: disable=C0103\n",
    "        tf = SummaryReader._get_tensorflow()\n",
    "        idx = 0\n",
    "        for tag, events in tag_to_events.items():\n",
    "            for e in events:\n",
    "                value = tf.make_ndarray(e.tensor_proto).item()\n",
    "                assert isinstance(value, bytes)\n",
    "                value = value.decode('utf-8')\n",
    "                cols['step'][idx] = e.step\n",
    "                if self._pivot:\n",
    "                    cols[tag][idx] = value\n",
    "                else:\n",
    "                    cols['tag'][idx] = tag\n",
    "                    cols['value'][idx] = value\n",
    "                idx += 1\n",
    "        return cols\n",
    "\n",
    "    def _parse_hparams(self, event_acc: EventAccumulator) -> \\\n",
    "            Tuple[List[str], Dict[str, Any]]:\n",
    "        \"\"\"Helper function for parsing tags and values of hparams.\"\"\"\n",
    "        # hparam info is in ssi tag\n",
    "        ssi_tag = PLUGIN_RAW_TAGS['hparams/ssi']\n",
    "        if ssi_tag not in self.get_raw_tags(HPARAMS, event_acc):\n",
    "            return [], {}\n",
    "        data = self.get_raw_events(HPARAMS, ssi_tag, event_acc)\n",
    "        plugin_data: HParamsPluginData = HParamsPluginData.FromString(data)\n",
    "        ssi = plugin_data.session_start_info\n",
    "        tags = list(ssi.hparams.keys())\n",
    "        values = {}\n",
    "        for tag in tags:\n",
    "            fields = ssi.hparams[tag].ListFields()\n",
    "            assert len(fields) == 1\n",
    "            assert len(fields[0]) == 2\n",
    "            values[tag] = [fields[0][1]]\n",
    "        return tags, values\n",
    "\n",
    "    def _get_default_cols(self, tag_to_events: Dict[str, ScalarEvent],\n",
    "                          wall_time=True) -> Dict[str, List[Any]]:\n",
    "        \"\"\"Get default entries based on the extra columns.\"\"\"\n",
    "        length = 0\n",
    "        for events in tag_to_events.values():\n",
    "            length += len(events)\n",
    "        cols: Dict[str, Any] = defaultdict(lambda: [np.NaN] * length)\n",
    "        if 'dir_name' in self._extra_columns:\n",
    "            cols['dir_name'] = [''] * length\n",
    "        if 'file_name' in self._extra_columns:\n",
    "            cols['file_name'] = [os.path.basename(self.log_path)] * length\n",
    "        if 'wall_time' not in self._extra_columns or not wall_time:\n",
    "            return cols\n",
    "        cols['wall_time'] = []\n",
    "        for events in tag_to_events.values():\n",
    "            cols['wall_time'].extend([e.wall_time for e in events])\n",
    "        # assert len(cols['wall_time']) == length\n",
    "        return cols\n",
    "\n",
    "    def _parse_events(self, event_type: str, event_acc: EventAccumulator):\n",
    "        \"\"\"Parse and store `event_type` events inside a event file.\n",
    "\n",
    "        :param event_acc: A loaded `EventAccumulator` for parsing events.\n",
    "        :type event_acc: EventAccumulator\n",
    "        :raises ValueError: if `log_path` is a directory.\n",
    "        \"\"\"\n",
    "        if os.path.isdir(self.log_path):\n",
    "            raise ValueError(f\"Not an event file: {self.log_path}\")\n",
    "        assert self._tags is not None\n",
    "        if event_type == HPARAMS:\n",
    "            self._tags[event_type], all_events = self._parse_hparams(event_acc)\n",
    "        else:\n",
    "            # parsed tags same as raw tags\n",
    "            self._tags[event_type] = cast(\n",
    "                List[str],\n",
    "                self.get_raw_tags(event_type, event_acc))\n",
    "            all_events = cast(\n",
    "                Dict[str, List[Any]],\n",
    "                self.get_raw_events(event_type, None, event_acc))\n",
    "        # Filter tags\n",
    "        if event_type == TENSORS:\n",
    "            # Filter tags here also filter the corresponding events\n",
    "            filtered_tags: List[str] = []\n",
    "            for tag in PLUGIN_TAGS:\n",
    "                tags = self.get_raw_tags(tag, event_acc)\n",
    "                filtered_tags.extend(tags)\n",
    "            self._tags[event_type] = \\\n",
    "                list(filter(lambda x: x not in filtered_tags,\n",
    "                            self._tags[event_type]))\n",
    "        # Add columns according to the event type\n",
    "        get_cols = {\n",
    "            SCALARS: self._get_scalar_cols,\n",
    "            TENSORS: self._get_tensor_cols,\n",
    "            HISTOGRAMS: self._get_histogram_cols,\n",
    "            IMAGES: self._get_image_cols,\n",
    "            AUDIO: self._get_audio_cols,\n",
    "            HPARAMS: self._get_hparam_cols,\n",
    "            TEXT: self._get_text_cols,\n",
    "        }[event_type]\n",
    "        tag_to_events = {}\n",
    "        for tag in self._tags[event_type]:\n",
    "            events = all_events[tag]\n",
    "            # Rename tags\n",
    "            if event_type == TEXT and tag.endswith(PLUGIN_RAW_TAGS[TEXT]):\n",
    "                # Remove tag suffix for torch & tensorboardX\n",
    "                tag = tag[:-len(PLUGIN_RAW_TAGS[TEXT])]\n",
    "            tag_to_events[tag] = events\n",
    "        cols = get_cols(tag_to_events)\n",
    "        # Reorder columns\n",
    "        for tag in ['wall_time', 'dir_name', 'file_name']:\n",
    "            if tag in cols:\n",
    "                tmp = cols[tag]\n",
    "                cols.pop(tag)\n",
    "                cols[tag] = tmp\n",
    "        self._events[event_type] = pd.DataFrame.from_dict(cols)\n",
    "\n",
    "    @property\n",
    "    def children(self) -> Dict[str, 'SummaryReader']:\n",
    "        \"\"\"Returns a list of references to the children `SummaryReader` s.\n",
    "        Since each child may have their own children, the underlying data\n",
    "        structure is actually a tree that mirrors the directories and files in\n",
    "        the file system.\n",
    "\n",
    "        :return: A `{childName: SummaryReader}` dictionary.\n",
    "        :rtype: Dict[str, 'SummaryReader']\n",
    "        \"\"\"\n",
    "        return self._children.copy()\n",
    "\n",
    "    @property\n",
    "    def raw_tags(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Returns a dictionary containing a list of raw tags for each raw\n",
    "        event type. This property is only supported when `log_path` is a\n",
    "        event file.\n",
    "\n",
    "        :return: A `{eventType: ['list', 'of', 'tags']}` dictionary.\n",
    "        :rtype: Dict[str, List[str]]\n",
    "        \"\"\"\n",
    "        return cast(Dict[str, List[str]], self.get_raw_tags())\n",
    "\n",
    "    def get_raw_tags(self, event_type: str = None,\n",
    "                     event_acc: EventAccumulator = None) -> \\\n",
    "            Union[List[str], Dict[str, List[str]]]:\n",
    "        \"\"\"Returns a list of raw tags for the specified raw event type. If\n",
    "        `event_type` is None, return a dictionary containing a list of raw\n",
    "        tags for each raw event type. This function is only supported when\n",
    "        `log_path` is a event file.\n",
    "\n",
    "        :param event_type: the event type to retrieve, None means return all, \\\n",
    "            defaults to None.\n",
    "        :type event_type: {None, 'images', 'audio', 'histograms', 'scalars', \\\n",
    "            'tensors', 'graph', 'meta_graph', 'run_metadata', 'hparams', \\\n",
    "            'text'}, optional\n",
    "        :raises ValueError: if `log_path` is a directory.\n",
    "        :raises ValueError: if `event_type` is unknown.\n",
    "        :return: A `['list', 'of', 'tags']` list, or a \\\n",
    "            `{eventType: ['list', 'of', 'tags']}` dictionary.\n",
    "        :rtype: List[str] | Dict[str, List[str]]\n",
    "        \"\"\"\n",
    "        if event_type not in {None, 'images', 'audio', 'histograms', 'scalars',\n",
    "                              'tensors', 'graph', 'meta_graph',\n",
    "                              'run_metadata', 'hparams', 'text'}:\n",
    "            raise ValueError(f\"Unknown event_type: {event_type}\")\n",
    "        if os.path.isdir(self.log_path):\n",
    "            raise ValueError(f\"Not an event file: {self.log_path}\")\n",
    "        if event_acc is None:\n",
    "            event_acc = EventAccumulator(\n",
    "                self.log_path, STORE_EVERYTHING_SIZE_GUIDANCE)\n",
    "            event_acc.Reload()\n",
    "        tags = event_acc.Tags()\n",
    "        for tag in PLUGIN_TAGS:\n",
    "            tags[tag] = []\n",
    "            # pylint: disable=W0212\n",
    "            if tag in event_acc._plugin_to_tag_to_content:\n",
    "                content = event_acc.PluginTagToContent(tag)\n",
    "                tags[tag] = list(content.keys())\n",
    "        if event_type is None:\n",
    "            return tags\n",
    "        return tags[event_type]\n",
    "\n",
    "    @property\n",
    "    def raw_events(self) -> Dict[str, Dict[str, List[Any]]]:\n",
    "        \"\"\"Returns a dictionary of dictionary containing a list of\n",
    "        raw events for each raw event type.\n",
    "        This property is only supported when `log_path` is a event file.\n",
    "\n",
    "        :return: A `{eventType: {tag: ['list', 'of', 'events']}}` dictionary.\n",
    "        :rtype: Dict[str, Dict[str, List[Any]]]\n",
    "        \"\"\"\n",
    "        return cast(Dict[str, Dict[str, List[Any]]], self.get_raw_events())\n",
    "\n",
    "    def get_raw_events(self, event_type: str = None, tag: str = None,\n",
    "                       event_acc: EventAccumulator = None) -> \\\n",
    "            Union[List[Any], Dict[str, List[Any]],\n",
    "                  Dict[str, Dict[str, List[Any]]]]:\n",
    "        \"\"\"Returns a list of raw events for the specified raw event type. If\n",
    "        `tag` is None, return a dictionary containing a list of raw events for\n",
    "        each raw event type. If `event_type` is None, return a dictionary of\n",
    "        dictionary containing a list of raw events for each raw event type.\n",
    "        This function is only supported when `log_path` is a event file.\n",
    "\n",
    "        :raises ValueError: if `log_path` is a directory.\n",
    "        :raises KeyError: if `event_type` is unknown.\n",
    "        :raises KeyError: If the `tag` is not found.\n",
    "        :return: A `['list', 'of', 'events']` list, or a \\\n",
    "            `{tag: ['list', 'of', 'events']}` dictionary, or a \\\n",
    "            `{eventType: {tag: ['list', 'of', 'events']}}` dictionary.\n",
    "        :rtype: List[Any] | Dict[str, List[Any]] | \\\n",
    "                Dict[str, Dict[str, List[Any]]]\n",
    "        \"\"\"\n",
    "        if os.path.isdir(self.log_path):\n",
    "            raise ValueError(f\"Not an event file: {self.log_path}\")\n",
    "        if event_acc is None:\n",
    "            event_acc = EventAccumulator(\n",
    "                self.log_path, STORE_EVERYTHING_SIZE_GUIDANCE)\n",
    "            event_acc.Reload()\n",
    "        if event_type is None:\n",
    "            # Return all event types by recursion\n",
    "            if tag is not None:\n",
    "                raise ValueError(\"tag shouldn't be set if event_type is None\")\n",
    "            lst = self._make_empty_dict([])\n",
    "            for t in list(lst.keys()):\n",
    "                # Collect events\n",
    "                events = self.get_raw_events(t, None, event_acc)\n",
    "                lst[t] = cast(Dict[str, List[Any]], events)\n",
    "            return lst  # dict of dict containing list of events\n",
    "        # Only collect the specified event type\n",
    "        get_events = {\n",
    "            SCALARS: event_acc.Scalars,\n",
    "            TENSORS: event_acc.Tensors,\n",
    "            HISTOGRAMS: event_acc.Histograms,\n",
    "            IMAGES: event_acc.Images,\n",
    "            AUDIO: event_acc.Audio,\n",
    "            HPARAMS: (lambda tag: event_acc.PluginTagToContent(HPARAMS)[tag]),\n",
    "            TEXT: event_acc.Tensors,\n",
    "        }[event_type]\n",
    "        if tag is not None:\n",
    "            return get_events(tag)  # list of events\n",
    "        ret = {}\n",
    "        for t in self.get_raw_tags(event_type, event_acc):\n",
    "            ret[t] = get_events(t)\n",
    "        return ret  # dict containing list of events\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_empty_dict(data) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a dictionary containing an empty list for each event type.\n",
    "\n",
    "        :return: A dictionary containing an empty list for each event type.\n",
    "        :rtype: Dict[str, Any]\n",
    "        \"\"\"\n",
    "        return {\n",
    "            IMAGES: copy.deepcopy(data),\n",
    "            AUDIO: copy.deepcopy(data),\n",
    "            HISTOGRAMS: copy.deepcopy(data),\n",
    "            SCALARS: copy.deepcopy(data),\n",
    "            # COMPRESSED_HISTOGRAMS: [],\n",
    "            TENSORS: copy.deepcopy(data),\n",
    "            # GRAPH: [],\n",
    "            # META_GRAPH: [],\n",
    "            # RUN_METADATA: [],\n",
    "            HPARAMS: copy.deepcopy(data),\n",
    "            TEXT: copy.deepcopy(data),\n",
    "        }\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Returns the string representation of the `SummaryWriter` instance.\n",
    "        Should be invoked by `repr(reader)`.\n",
    "\n",
    "        :return: The string representation of the `SummaryWriter` instance.\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        return f\"SummaryReader(log_path='{self.log_path}')\"\n",
    "\n",
    "    def __getitem__(self, child_idx) -> 'SummaryReader':\n",
    "        \"\"\"Returns the child `SummaryReader` with index `child_idx`. Should\n",
    "        be invoked by `reader[idx]`.\n",
    "\n",
    "        :return: The child `SummaryReader` with index `child_idx`.\n",
    "        :rtype: SummaryReader\n",
    "        \"\"\"\n",
    "        return self.children[child_idx]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T10:39:05.519580344Z",
     "start_time": "2023-07-04T10:39:05.444069814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
